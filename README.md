# Semantic Segmentation: Encoder-Decoder vs Transformers

Для задачи семантической сегментации классическим решением является применение encoder-decoder архитектуры. Однако появление трансформеров показало, что количество обучаемых параметров в классических CNN-моделях избыточно. В нашем эксперименте мы сравним encoder-decoder подход с трансформерной архитектурой.

## Эксперимент

В качестве датасета мы используем **PASCAL VOC**. В качестве классической encoder-decoder модели возьмём **U-Net**.

Эмпирически установлено, что для обучения U-Net с нуля до получения приемлемых метрик (соответствующих baseline-решению задачи) требуется слишком много вычислительных ресурсов. Поэтому в качестве backbone для энкодерной части модели будем использовать **ResNet50**.

**Общее количество параметров:** 71 863 765

Для сравнения используем предобученную модель **SegFormer-B0**, которая сочетает трансформер с multilayer perceptron (MLP) декодером.

**Обучаемые параметры:** 3 719 541

![experiment 1](https://github.com/stainlao/cv_segmentation_enc-dec_vs_mit/blob/main/sources/comprehensive_model_comparison%20(1).png)

SegFormer использует Hierarchical Transformer Encoder, в котором self-attention помогает объединять информацию на разных уровнях обработки. Благодаря этому модель учитывает глобальный контекст фотографии. Кроме того, вместо глобального attention, как в обычных ViT, используется локальный, который оптимизирует количество обучаемых параметров и скорость вычислений.

В декодерной части SegFormer использует MLP-декодер, оптимизированный за счёт линейной проекции признаков с разных уровней энкодера MiT в общий hidden state. Все признаки выравниваются по размеру feature map через bilinear upsampling и конкатенируются, после чего обрабатываются через лёгкий многослойный перцептрон. Отказ от сверточных блоков и позиционных эмбеддингов значительно снижает количество параметров и ускоряет инференс без потери точности.

Такая архитектура позволяет в 23 раза снизить количество обучаемых параметров, а значит, и утилизируемой памяти GPU, но всего в 2 раза, так как в памяти GPU хранятся и состояния оптимизаторов, и тензоры входных данных, прочий кэш.

Благодаря оптимизациям SegFormer и учится быстрее, примерно в 3 раза.
![experiment 2](https://github.com/stainlao/cv_segmentation_enc-dec_vs_mit/blob/main/sources/SegFormer_epoch_10_prediction.png)


![experiment 2](https://github.com/stainlao/cv_segmentation_enc-dec_vs_mit/blob/main/sources/Pretrained%20U-Net%20(ResNet50)_epoch_10_prediction.png)

SegFormer показывает лучшие результаты по метрикам: 0.42 mIoU против 0.14 mIoU у U-Net. Однако стоит учитывать, что эксперимент проводился на ограниченных вычислительных ресурсах при небольшом количестве эпох обучения (всего 10). В дополнительном эксперименте U-Net обучался 30 эпох и достиг mIoU 0.3276, что соответствует результатам SegFormer после 6 эпох обучения.
![experiment 2](https://github.com/stainlao/cv_segmentation_enc-dec_vs_mit/blob/main/sources/comprehensive_model_comparison.png)

![experiment 2](https://github.com/stainlao/cv_segmentation_enc-dec_vs_mit/blob/main/sources/Pretrained%20U-Net%20(ResNet50)_epoch_30_prediction.png)


Что еще хотелось бы сделать: поучить обе модели до переобучения, сравнить метрики, какой максимальный mIoU удастся выжать из каждой из них. Однако для сравнения максимального качества моделей нужно перебирать кучу гиперпараметров, экспериментировать с разными оптимизаторами — что долго.

Таким образом, SegFormer, благодаря использованию новых оптимизированных подходов к решению задачи, демонстрирует значимо лучший перформанс по всем показателям.
