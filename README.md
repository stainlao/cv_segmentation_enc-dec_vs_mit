# Semantic Segmentation: Encoder-Decoder vs Transformers
Для задачи семантической сегментации классическим решением является применение encoder-decoder модели. Однако, появление трансформеров показало, что количество обучаемых параметров классических CNN избыточно. В ходе нашего эксперимента мы сравним Encoder-Decoder модель с моделью, основанной на трансформере.

## Эксперимент

В качестве датасета будет использовать PASCAL VOC, в качестве популярной для данной задачи Encoder-Decoder модели возьмем U-Net.

Эмпирически выявлено, что для обучения под данную задачу U-Net c нуля до получения +- адекватных метрик, ожидаемых от baseline решения задачи, требуется слишком много компьюта. Поэтому в качестве backbone для энкодерной части модели возьмем resnet50.

Общее количество параметров: 71,863,765

Для сравнения будем использовать предобученный SegFormer-bo, который объединяет Transformer с multilayer perceptron (MLP) декодерами.

Обучаемые параметры: 3,719,541

![experiment 1](https://github.com/stainlao/cv_segmentation_enc-dec_vs_mit/blob/main/sources/comprehensive_model_comparison%20(1).png)

SegFormer использует Hierarchical Transformer Encoder, в котором self-attention помогает объединять информацию на разных уровнях обработки. Благодаря этому модель учитывает глобальный контекст фотографии. Кроме того, вместо глобального attention как в обычных ViT используется локальный, который оптимизирует количество обучаемых параметров и скорость вычислений.

В декодерной части SegFormer использует MLP-декодер, оптимизированный за счёт линейной проекции признаков с разных уровней энкодера MiT в общий hidden state. Все признаки выравниваются по размеру feature map через bilinear upsampling и конкатенируются, после чего обрабатываются через лёгкий многослойный перцептрон. Отказ от сверточных блоков и позиционных эмбеддингов значительно снижает количество параметров и ускоряет инференс без потери точности.

Такая архитектура позволяет в 23 раза снизить количество обучаемых параметров, а значит и утилизируемой памяти gpu, но всего в 2 раза, т.к. в памяти gpu хранятся и состояния оптимизаторов, и тензоры входных данных, прочий кэш.

Благодаря оптимизациям SegFormer и учится быстрее, примерно в 3 раза.

По метрикам тоже побеждает SegFormer, 0.42 vs 0.14 mIoU. Однако, в рамках ограниченного компьюта мы учим модели не долго(всего 10 эпох). Дополнительно (эксперимент 2) я учил u-net 30 эпох и получил mIoU 0.3276, что соответствует результатам 6й эпохи обучения SegFormer.

Что еще хотелось бы сделать: поучить обе модели до переобучения, сравнить метрики, какой максимальный mIoU удастся выжать из каждой из них. Однако для сравнения максимального качества моделей нужно перебирать кучу гиперпараметров, экспериментировать с разными оптимизаторами, что долго.

Таким образом, SegFormer благодаря использованию новых, оптимизированных подходов к решению задачи демонстрирует значимо лучший перформанс по всем показателям.
