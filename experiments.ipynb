{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AGzrwRvdxnoI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Segmentation: Encoder-Decoder vs Transformers\n",
        "Для задачи семантической сегментации классическим решением является применение encoder-decoder модели. Однако, появление трансформеров показало, что количество обучаемых параметров классических CNN избыточно. В ходе нашего эксперимента мы сравним Encoder-Decoder модель с моделью, основанной на трансформере.\n",
        "\n",
        "## Эксперимент\n",
        "\n",
        "В качестве датасета будет использовать PASCAL VOC, в качестве популярной для данной задачи Encoder-Decoder модели возьмем U-Net.\n",
        "\n",
        "Эмпирически выявлено, что для обучения под данную задачу U-Net c нуля до получения +- адекватных метрик, ожидаемых от baseline решения задачи, требуется слишком много компьюта. Поэтому в качестве backbone для энкодерной части модели возьмем resnet50.\n",
        "\n",
        "Общее количество параметров: 71,863,765\n",
        "\n",
        "Для сравнения будем использовать предобученный SegFormer-bo, который объединяет Transformer с multilayer perceptron (MLP) декодерами.\n",
        "\n",
        "Обучаемые параметры: 3,719,541\n",
        "\n",
        "SegFormer использует Hierarchical Transformer Encoder, в котором self-attention помогает объединять информацию на разных уровнях обработки. Благодаря этому модель учитывает глобальный контекст фотографии. Кроме того, вместо глобального attention как в обычных ViT используется локальный, который оптимизирует количество обучаемых параметров и скорость вычислений.\n",
        "\n",
        "В декодерной части SegFormer использует MLP-декодер, оптимизированный за счёт линейной проекции признаков с разных уровней энкодера MiT в общий hidden state. Все признаки выравниваются по размеру feature map через bilinear upsampling и конкатенируются, после чего обрабатываются через лёгкий многослойный перцептрон. Отказ от сверточных блоков и позиционных эмбеддингов значительно снижает количество параметров и ускоряет инференс без потери точности.\n",
        "\n",
        "Такая архитектура позволяет в 23 раза снизить количество обучаемых параметров, а значит и утилизируемой памяти gpu, но всего в 2 раза, т.к. в памяти gpu хранятся и состояния оптимизаторов, и тензоры входных данных, прочий кэш.\n",
        "\n",
        "Благодаря оптимизациям SegFormer и учится быстрее, примерно в 3 раза.\n",
        "\n",
        "По метрикам тоже побеждает SegFormer, 0.42 vs 0.14 mIoU. Однако, в рамках ограниченного компьюта мы учим модели не долго(всего 10 эпох). Дополнительно (эксперимент 2) я учил u-net 30 эпох и получил mIoU 0.3276, что соответствует результатам 6й эпохи обучения SegFormer.\n",
        "\n",
        "Что еще хотелось бы сделать: поучить обе модели до переобучения, сравнить метрики, какой максимальный mIoU удастся выжать из каждой из них. Однако для сравнения максимального качества моделей нужно перебирать кучу гиперпараметров, экспериментировать с разными оптимизаторами, что долго.\n",
        "\n",
        "Таким образом, SegFormer благодаря использованию новых, оптимизированных подходов к решению задачи демонстрирует значимо лучший перформанс по всем показателям."
      ],
      "metadata": {
        "id": "FwEswv276pHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compare encoder-decoder vs transformer"
      ],
      "metadata": {
        "id": "_jeAoTnn4Hxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import VOCSegmentation\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class PretrainedUNet(nn.Module):\n",
        "    \"\"\"U-Net with pretrained ResNet encoder for better transfer learning\"\"\"\n",
        "    def __init__(self, n_classes=21, backbone='resnet50', pretrained=True):\n",
        "        super(PretrainedUNet, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        if backbone == 'resnet50':\n",
        "            resnet = models.resnet50(pretrained=pretrained)\n",
        "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
        "\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            resnet.conv1,\n",
        "            resnet.bn1,\n",
        "            resnet.relu\n",
        "        )  # 64 channels\n",
        "\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            resnet.maxpool,\n",
        "            resnet.layer1\n",
        "        )\n",
        "\n",
        "        self.encoder3 = resnet.layer2\n",
        "        self.encoder4 = resnet.layer3\n",
        "        self.encoder5 = resnet.layer4\n",
        "\n",
        "        self.decoder4 = self._make_decoder_block(encoder_channels[4], encoder_channels[3])\n",
        "        self.decoder3 = self._make_decoder_block(encoder_channels[3], encoder_channels[2])\n",
        "        self.decoder2 = self._make_decoder_block(encoder_channels[2], encoder_channels[1])\n",
        "        self.decoder1 = self._make_decoder_block(encoder_channels[1], encoder_channels[0])\n",
        "\n",
        "        self.final_conv = nn.Conv2d(encoder_channels[0], n_classes, 1)\n",
        "\n",
        "        self._initialize_decoder_weights()\n",
        "\n",
        "    def _make_decoder_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 2, stride=2),\n",
        "            DoubleConv(out_channels * 2, out_channels)  # *2 for skip connection\n",
        "        )\n",
        "\n",
        "    def _initialize_decoder_weights(self):\n",
        "        for m in [self.decoder4, self.decoder3, self.decoder2, self.decoder1, self.final_conv]:\n",
        "            for module in m.modules():\n",
        "                if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "                elif isinstance(module, nn.BatchNorm2d):\n",
        "                    nn.init.constant_(module.weight, 1)\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder with skip connections\n",
        "        e1 = self.encoder1(x)      # 64, H/2, W/2\n",
        "        e2 = self.encoder2(e1)     # 256, H/4, W/4\n",
        "        e3 = self.encoder3(e2)     # 512, H/8, W/8\n",
        "        e4 = self.encoder4(e3)     # 1024, H/16, W/16\n",
        "        e5 = self.encoder5(e4)     # 2048, H/32, W/32\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        d4 = self.decoder4[0](e5)  # Upsample\n",
        "        d4 = torch.cat([d4, e4], dim=1)  # Skip connection\n",
        "        d4 = self.decoder4[1](d4)  # Double conv\n",
        "\n",
        "        d3 = self.decoder3[0](d4)\n",
        "        d3 = torch.cat([d3, e3], dim=1)\n",
        "        d3 = self.decoder3[1](d3)\n",
        "\n",
        "        d2 = self.decoder2[0](d3)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.decoder2[1](d2)\n",
        "\n",
        "        d1 = self.decoder1[0](d2)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.decoder1[1](d1)\n",
        "\n",
        "        d1 = F.interpolate(d1, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return self.final_conv(d1)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"Original U-Net implementation (from scratch)\"\"\"\n",
        "    def __init__(self, n_channels=3, n_classes=21):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Encoder\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n",
        "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n",
        "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512))\n",
        "        self.down4 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(512, 1024))\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.conv1 = DoubleConv(512 + 512, 512)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.conv2 = DoubleConv(256 + 256, 256)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.conv3 = DoubleConv(128 + 128, 128)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.conv4 = DoubleConv(64 + 64, 64)\n",
        "\n",
        "        # Final output layer\n",
        "        self.outc = nn.Conv2d(64, n_classes, 1)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        x = self.up1(x5)\n",
        "        if x.shape[-2:] != x4.shape[-2:]:\n",
        "            x = F.interpolate(x, size=x4.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x4, x], dim=1)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.up2(x)\n",
        "        if x.shape[-2:] != x3.shape[-2:]:\n",
        "            x = F.interpolate(x, size=x3.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x3, x], dim=1)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = self.up3(x)\n",
        "        if x.shape[-2:] != x2.shape[-2:]:\n",
        "            x = F.interpolate(x, size=x2.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x2, x], dim=1)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        x = self.up4(x)\n",
        "        if x.shape[-2:] != x1.shape[-2:]:\n",
        "            x = F.interpolate(x, size=x1.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x1, x], dim=1)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        return self.outc(x)\n",
        "\n",
        "class VOCDataset(Dataset):\n",
        "    def __init__(self, split='train', transform=None, target_transform=None):\n",
        "        self.dataset = VOCSegmentation(\n",
        "            root='./data',\n",
        "            year='2012',\n",
        "            image_set=split,\n",
        "            download=True,\n",
        "            transform=transform,\n",
        "            target_transform=target_transform\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, target = self.dataset[idx]\n",
        "\n",
        "        if not isinstance(image, torch.Tensor):\n",
        "            image = transforms.ToTensor()(image)\n",
        "\n",
        "        if not isinstance(target, torch.Tensor):\n",
        "            target = torch.from_numpy(np.array(target)).long()\n",
        "            target[target == 255] = 0\n",
        "\n",
        "        return image, target\n",
        "\n",
        "def get_transforms():\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    target_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256), interpolation=Image.NEAREST),\n",
        "    ])\n",
        "\n",
        "    return train_transform, target_transform\n",
        "\n",
        "class IoUMetrics:\n",
        "    def __init__(self, num_classes, device):\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.intersection = torch.zeros(self.num_classes, device=self.device)\n",
        "        self.union = torch.zeros(self.num_classes, device=self.device)\n",
        "        self.total_pixels = 0\n",
        "        self.correct_pixels = 0\n",
        "\n",
        "    def update(self, pred, target):\n",
        "        pred = pred.flatten()\n",
        "        target = target.flatten()\n",
        "\n",
        "        valid_mask = target != 255\n",
        "        pred = pred[valid_mask]\n",
        "        target = target[valid_mask]\n",
        "\n",
        "        self.correct_pixels += (pred == target).sum().item()\n",
        "        self.total_pixels += target.numel()\n",
        "\n",
        "        for cls in range(self.num_classes):\n",
        "            pred_cls = (pred == cls)\n",
        "            target_cls = (target == cls)\n",
        "\n",
        "            intersection = (pred_cls & target_cls).sum().float()\n",
        "            union = (pred_cls | target_cls).sum().float()\n",
        "\n",
        "            self.intersection[cls] += intersection\n",
        "            self.union[cls] += union\n",
        "\n",
        "    def compute(self):\n",
        "        iou_per_class = self.intersection / (self.union + 1e-8)\n",
        "        valid_classes = self.union > 0\n",
        "        mean_iou = iou_per_class[valid_classes].mean()\n",
        "        pixel_accuracy = self.correct_pixels / (self.total_pixels + 1e-8)\n",
        "\n",
        "        return {\n",
        "            'mean_iou': mean_iou.item(),\n",
        "            'iou_per_class': iou_per_class.cpu().numpy(),\n",
        "            'pixel_accuracy': pixel_accuracy,\n",
        "            'valid_classes': valid_classes.cpu().numpy()\n",
        "        }\n",
        "\n",
        "def get_memory_usage():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated(device) / 1024 / 1024\n",
        "    else:\n",
        "        process = psutil.Process(os.getpid())\n",
        "        return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device, model_name, num_classes=21):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    iou_metrics = IoUMetrics(num_classes, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            if 'segformer' in model_name.lower():\n",
        "                outputs = model(pixel_values=images)\n",
        "                logits = outputs.logits\n",
        "            else:\n",
        "                logits = model(images)\n",
        "\n",
        "            logits = F.interpolate(logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
        "            loss = criterion(logits, masks)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            pred_masks = torch.argmax(logits, dim=1)\n",
        "            iou_metrics.update(pred_masks, masks)\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    metrics = iou_metrics.compute()\n",
        "\n",
        "    return avg_loss, metrics\n",
        "\n",
        "def train_epoch(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epoch, model_name, num_classes=21):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    start_memory = get_memory_usage()\n",
        "\n",
        "    for i, (images, masks) in enumerate(train_loader):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if 'segformer' in model_name.lower():\n",
        "            outputs = model(pixel_values=images)\n",
        "            logits = outputs.logits\n",
        "            logits = F.interpolate(logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        else:\n",
        "            logits = model(images)\n",
        "            logits = F.interpolate(logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        loss = criterion(logits, masks)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        if 'unet' in model_name.lower():\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f'Epoch {epoch+1}, Batch {i}, Loss: {loss.item():.4f}, LR: {current_lr:.6f}')\n",
        "\n",
        "    # Step scheduler after epoch\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    epoch_time = time.time() - start_time\n",
        "    end_memory = get_memory_usage()\n",
        "    memory_used = end_memory - start_memory\n",
        "\n",
        "    print(f\"Evaluating {model_name} on validation set...\")\n",
        "    val_loss, val_metrics = evaluate_model(model, val_loader, criterion, device, model_name, num_classes)\n",
        "    print(f\"Validation - Loss: {val_loss:.4f}, mIoU: {val_metrics['mean_iou']:.4f}, \"\n",
        "          f\"Pixel Acc: {val_metrics['pixel_accuracy']:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'val_metrics': val_metrics,\n",
        "        'epoch_time': epoch_time,\n",
        "        'memory_used': memory_used\n",
        "    }\n",
        "\n",
        "def visualize_predictions(model, dataloader, device, epoch, model_name, save_path, num_classes=21):\n",
        "    model.eval()\n",
        "    data_iter = iter(dataloader)\n",
        "    images, true_masks = next(data_iter)\n",
        "    image = images[0:1].to(device)\n",
        "    true_mask = true_masks[0].cpu().numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if 'segformer' in model_name.lower():\n",
        "            outputs = model(pixel_values=image)\n",
        "            logits = outputs.logits\n",
        "        else:\n",
        "            logits = model(image)\n",
        "\n",
        "        logits = F.interpolate(logits, size=true_mask.shape, mode='bilinear', align_corners=False)\n",
        "        pred_mask = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "    iou_metrics = IoUMetrics(num_classes, device='cpu')\n",
        "    iou_metrics.update(torch.tensor(pred_mask), torch.tensor(true_mask))\n",
        "    metrics = iou_metrics.compute()\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    original_img = images[0].cpu()\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    original_img = torch.clamp(original_img * std + mean, 0, 1)\n",
        "\n",
        "    axes[0].imshow(original_img.permute(1, 2, 0))\n",
        "    axes[0].set_title('Original Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(true_mask, cmap='tab20')\n",
        "    axes[1].set_title('Ground Truth Mask')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(pred_mask, cmap='tab20')\n",
        "    axes[2].set_title(f'{model_name} - Epoch {epoch+1}\\n'\n",
        "                     f'mIoU: {metrics[\"mean_iou\"]:.3f}, '\n",
        "                     f'Pixel Acc: {metrics[\"pixel_accuracy\"]:.3f}')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_path}/{model_name}_epoch_{epoch+1}_prediction.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def setup_model_training(model_name, model, freeze_encoder=False):\n",
        "    \"\"\"Setup training configuration for different models\"\"\"\n",
        "    if model_name == 'Pretrained U-Net (ResNet50)':\n",
        "        if freeze_encoder:\n",
        "            # Freeze encoder (ResNet backbone) for initial training\n",
        "            for name, param in model.named_parameters():\n",
        "                if 'encoder' in name:\n",
        "                    param.requires_grad = False\n",
        "            lr = 1e-3  # Higher LR for decoder only\n",
        "            print(f\"Frozen encoder parameters in {model_name}\")\n",
        "        else:\n",
        "            # Fine-tune entire model\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = True\n",
        "            lr = 5e-4  # Lower LR for fine-tuning\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
        "\n",
        "    elif model_name == 'U-Net (From Scratch)':\n",
        "        lr = 6e-4\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
        "\n",
        "    else:  # SegFormer\n",
        "        lr = 1e-4\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        scheduler = None\n",
        "\n",
        "    return optimizer, scheduler\n",
        "\n",
        "def run_comparison():\n",
        "    train_transform, target_transform = get_transforms()\n",
        "\n",
        "    print(\"Loading VOC2012 dataset...\")\n",
        "    train_dataset = VOCDataset('train', train_transform, target_transform)\n",
        "    val_dataset = VOCDataset('val', train_transform, target_transform)\n",
        "\n",
        "    batch_size = 16\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    os.makedirs('segmentation_results', exist_ok=True)\n",
        "\n",
        "    print(\"\\nInitializing models...\")\n",
        "\n",
        "    # Initialize models\n",
        "    unet_scratch = UNet(n_channels=3, n_classes=21).to(device)\n",
        "    unet_pretrained_resnet50 = PretrainedUNet(n_classes=21, backbone='resnet50', pretrained=True).to(device)\n",
        "\n",
        "    segformer_model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "        \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
        "        num_labels=21,\n",
        "        ignore_mismatched_sizes=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Count parameters\n",
        "    models_info = [\n",
        "        #('U-Net (From Scratch)', unet_scratch),\n",
        "        ('Pretrained U-Net (ResNet50)', unet_pretrained_resnet50),\n",
        "        ('SegFormer', segformer_model)\n",
        "    ]\n",
        "\n",
        "    for name, model in models_info:\n",
        "        params = count_parameters(model)\n",
        "        print(f\"{name} parameters: {params:,} ({params/1e6:.1f}M)\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "    epochs = 10\n",
        "    num_classes = 21\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model_name, model in models_info:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training {model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        optimizer, scheduler = setup_model_training(model_name, model)\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        val_mious = []\n",
        "        val_pixel_accs = []\n",
        "        train_times = []\n",
        "        memory_usage = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "            epoch_results = train_epoch(\n",
        "                model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                device, epoch, model_name, num_classes\n",
        "            )\n",
        "\n",
        "            train_losses.append(epoch_results['train_loss'])\n",
        "            val_losses.append(epoch_results['val_loss'])\n",
        "            val_mious.append(epoch_results['val_metrics']['mean_iou'])\n",
        "            val_pixel_accs.append(epoch_results['val_metrics']['pixel_accuracy'])\n",
        "            train_times.append(epoch_results['epoch_time'])\n",
        "            memory_usage.append(epoch_results['memory_used'])\n",
        "\n",
        "            visualize_predictions(model, val_loader, device, epoch, model_name, 'segmentation_results', num_classes)\n",
        "\n",
        "            print(f\"Train Loss: {epoch_results['train_loss']:.4f}, \"\n",
        "                  f\"Val Loss: {epoch_results['val_loss']:.4f}, \"\n",
        "                  f\"Time: {epoch_results['epoch_time']:.2f}s, \"\n",
        "                  f\"Memory: {epoch_results['memory_used']:.2f}MB\")\n",
        "\n",
        "        param_count = count_parameters(model)\n",
        "        results[model_name] = {\n",
        "            'parameters': param_count,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'val_mious': val_mious,\n",
        "            'val_pixel_accs': val_pixel_accs,\n",
        "            'train_times': train_times,\n",
        "            'memory_usage': memory_usage,\n",
        "            'avg_time_per_epoch': np.mean(train_times),\n",
        "            'avg_memory_per_epoch': np.mean(memory_usage),\n",
        "            'final_train_loss': train_losses[-1],\n",
        "            'final_val_loss': val_losses[-1],\n",
        "            'best_miou': max(val_mious),\n",
        "            'final_miou': val_mious[-1],\n",
        "            'best_pixel_acc': max(val_pixel_accs),\n",
        "            'final_pixel_acc': val_pixel_accs[-1]\n",
        "        }\n",
        "\n",
        "\n",
        "    print(\"\\nGenerating comparison plots...\")\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(24, 12))\n",
        "\n",
        "    colors = ['blue', 'green', 'red', 'orange']\n",
        "\n",
        "    # Loss comparison\n",
        "    for i, model_name in enumerate(results.keys()):\n",
        "        axes[0, 0].plot(results[model_name]['train_losses'],\n",
        "                       label=f'{model_name} (Train)', marker='o', color=colors[i])\n",
        "        axes[0, 0].plot(results[model_name]['val_losses'],\n",
        "                       label=f'{model_name} (Val)', marker='s', linestyle='--', color=colors[i])\n",
        "    axes[0, 0].set_title('Loss Comparison')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    # IoU comparison\n",
        "    for i, model_name in enumerate(results.keys()):\n",
        "        axes[0, 1].plot(results[model_name]['val_mious'],\n",
        "                       label=model_name, marker='o', color=colors[i])\n",
        "    axes[0, 1].set_title('Mean IoU Comparison')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Mean IoU')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    # Pixel Accuracy comparison\n",
        "    for i, model_name in enumerate(results.keys()):\n",
        "        axes[0, 2].plot(results[model_name]['val_pixel_accs'],\n",
        "                       label=model_name, marker='o', color=colors[i])\n",
        "    axes[0, 2].set_title('Pixel Accuracy Comparison')\n",
        "    axes[0, 2].set_xlabel('Epoch')\n",
        "    axes[0, 2].set_ylabel('Pixel Accuracy')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True)\n",
        "\n",
        "    # Time comparison\n",
        "    model_names = list(results.keys())\n",
        "    times = [results[name]['avg_time_per_epoch'] for name in model_names]\n",
        "    bars = axes[1, 0].bar(range(len(model_names)), times, color=colors[:len(model_names)])\n",
        "    axes[1, 0].set_title('Average Training Time per Epoch')\n",
        "    axes[1, 0].set_ylabel('Time (seconds)')\n",
        "    axes[1, 0].set_xticks(range(len(model_names)))\n",
        "    axes[1, 0].set_xticklabels([name.replace(' ', '\\n') for name in model_names], rotation=0)\n",
        "    for i, v in enumerate(times):\n",
        "        axes[1, 0].text(i, v + max(times)*0.01, f'{v:.1f}s', ha='center')\n",
        "\n",
        "    # Memory comparison\n",
        "    memory = [results[name]['avg_memory_per_epoch'] for name in model_names]\n",
        "    bars = axes[1, 1].bar(range(len(model_names)), memory, color=colors[:len(model_names)])\n",
        "    axes[1, 1].set_title('Average Memory Usage per Epoch')\n",
        "    axes[1, 1].set_ylabel('Memory (MB)')\n",
        "    axes[1, 1].set_xticks(range(len(model_names)))\n",
        "    axes[1, 1].set_xticklabels([name.replace(' ', '\\n') for name in model_names], rotation=0)\n",
        "    for i, v in enumerate(memory):\n",
        "        axes[1, 1].text(i, v + max(memory)*0.01, f'{v:.1f}MB', ha='center')\n",
        "\n",
        "    # Parameters comparison\n",
        "    params = [results[name]['parameters'] for name in model_names]\n",
        "    axes[1, 2].bar(model_names, params, color=['purple', 'brown'])\n",
        "    axes[1, 2].set_title('Model Parameters Comparison')\n",
        "    axes[1, 2].set_ylabel('Number of Parameters')\n",
        "    for i, v in enumerate(params):\n",
        "        axes[1, 2].text(i, v + max(params)*0.01, f'{v/1e6:.1f}M', ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('segmentation_results/comprehensive_model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Create IoU metrics summary plot\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.35\n",
        "\n",
        "    best_mious = [results[name]['best_miou'] for name in model_names]\n",
        "    final_mious = [results[name]['final_miou'] for name in model_names]\n",
        "\n",
        "    bars1 = ax.bar(x - width/2, best_mious, width, label='Best mIoU', color='skyblue')\n",
        "    bars2 = ax.bar(x + width/2, final_mious, width, label='Final mIoU', color='lightcoral')\n",
        "\n",
        "    ax.set_xlabel('Models')\n",
        "    ax.set_ylabel('Mean IoU')\n",
        "    ax.set_title('IoU Performance Comparison')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(model_names)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "                f'{height:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "                f'{height:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('segmentation_results/iou_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for model_name in results.keys():\n",
        "        result = results[model_name]\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  Parameters: {result['parameters']:,} ({result['parameters']/1e6:.1f}M)\")\n",
        "        print(f\"  Final Train Loss: {result['final_train_loss']:.4f}\")\n",
        "        print(f\"  Final Val Loss: {result['final_val_loss']:.4f}\")\n",
        "        print(f\"  Best mIoU: {result['best_miou']:.4f}\")\n",
        "        print(f\"  Final mIoU: {result['final_miou']:.4f}\")\n",
        "        print(f\"  Best Pixel Accuracy: {result['best_pixel_acc']:.4f}\")\n",
        "        print(f\"  Final Pixel Accuracy: {result['final_pixel_acc']:.4f}\")\n",
        "        print(f\"  Avg Time/Epoch: {result['avg_time_per_epoch']:.2f}s\")\n",
        "        print(f\"  Avg Memory/Epoch: {result['avg_memory_per_epoch']:.2f}MB\")\n",
        "\n",
        "    print(f\"\\nDataset: PASCAL VOC 2012 (21 classes)\")\n",
        "    print(f\"Input Resolution: 256x256\")\n",
        "    print(f\"Batch Size: {batch_size}\")\n",
        "    print(f\"Training Epochs: {epochs}\")\n",
        "    print(f\"Evaluation Metrics: Loss, Mean IoU, Pixel Accuracy\")\n",
        "\n",
        "    # Performance ranking\n",
        "    print(f\"\\n\" + \"=\"*50)\n",
        "    print(\"PERFORMANCE RANKING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Sort by best mIoU\n",
        "    sorted_by_miou = sorted(results.items(), key=lambda x: x[1]['best_miou'], reverse=True)\n",
        "    print(\"\\nBy Best Mean IoU:\")\n",
        "    for i, (model_name, result) in enumerate(sorted_by_miou, 1):\n",
        "        print(f\"  {i}. {model_name}: {result['best_miou']:.4f}\")\n",
        "\n",
        "    # Sort by efficiency (mIoU per parameter)\n",
        "    efficiency_scores = {name: result['best_miou'] / (result['parameters'] / 1e6)\n",
        "                        for name, result in results.items()}\n",
        "    sorted_by_efficiency = sorted(efficiency_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(\"\\nBy Efficiency (mIoU per Million Parameters):\")\n",
        "    for i, (model_name, score) in enumerate(sorted_by_efficiency, 1):\n",
        "        print(f\"  {i}. {model_name}: {score:.4f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "HRcpue-6xkgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_layers(model, model_name):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Модель: {model_name}\")\n",
        "    print(f\"{'Слой':<60} {'Тип':<30} {'Обучаемые параметры':<20}\")\n",
        "    print('-' * 110)\n",
        "\n",
        "    # Функция для точного подсчёта параметров\n",
        "    def count_parameters(module):\n",
        "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "    total_trainable = 0\n",
        "    total_non_trainable = 0\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        # Пропускаем корневой модуль (его параметры уже включены в дочерние)\n",
        "        if name == \"\":\n",
        "            continue\n",
        "\n",
        "        # Считаем параметры только для этого модуля (без рекурсии)\n",
        "        params = sum(p.numel() for p in module.parameters(recurse=False))\n",
        "        if params == 0:\n",
        "            continue\n",
        "\n",
        "        # Считаем обучаемые параметры\n",
        "        trainable = count_parameters(module)\n",
        "        non_trainable = params - trainable\n",
        "\n",
        "        # Форматируем имя слоя\n",
        "        depth = name.count('.')\n",
        "        indent = '│   ' * depth + '├── ' if depth > 0 else ''\n",
        "        display_name = f\"{indent}{name.split('.')[-1]}\"\n",
        "\n",
        "        print(f\"{display_name:<60} {module.__class__.__name__:<30} {trainable} ({trainable/params:.1%})\")\n",
        "\n",
        "        total_trainable += trainable\n",
        "        total_non_trainable += non_trainable\n",
        "\n",
        "    # Подсчитываем общее количество параметров модели\n",
        "    total_params = total_trainable + total_non_trainable\n",
        "\n",
        "    print('-' * 110)\n",
        "    print(f\"{'ВСЕГО:':<60} {'':<30} {total_trainable} ({total_trainable/total_params:.1%})\")\n",
        "    print(f\"Общее количество параметров: {total_params:,}\")\n",
        "    print(f\"Обучаемые параметры: {total_trainable:,} ({total_trainable/total_params:.1%})\")\n",
        "\n",
        "# Initialize models\n",
        "unet_scratch = UNet(n_channels=3, n_classes=21).to(device)\n",
        "unet_pretrained_resnet50 = PretrainedUNet(n_classes=21, backbone='resnet50', pretrained=True).to(device)\n",
        "\n",
        "segformer_model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "    \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
        "    num_labels=21,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "models_info = [\n",
        "    #('U-Net (From Scratch)', unet_scratch),\n",
        "    ('Pretrained U-Net (ResNet50)', unet_pretrained_resnet50),\n",
        "    ('SegFormer', segformer_model)\n",
        "]\n",
        "\n",
        "\n",
        "# Применяем к вашим моделям\n",
        "print_model_layers(unet_pretrained_resnet50, \"PretrainedUNet (resnet50)\")\n",
        "print_model_layers(segformer_model, \"Segformer-b0\")"
      ],
      "metadata": {
        "id": "vRnizyf5xszW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WODLeFrgNCX7"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    results = run_comparison()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## unet learning solo more epochs"
      ],
      "metadata": {
        "id": "AGzrwRvdxnoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class PretrainedUNet(nn.Module):\n",
        "    \"\"\"U-Net with pretrained ResNet encoder for better transfer learning\"\"\"\n",
        "    def __init__(self, n_classes=21, backbone='resnet50', pretrained=True):\n",
        "        super(PretrainedUNet, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        if backbone == 'resnet50':\n",
        "            resnet = models.resnet50(pretrained=pretrained)\n",
        "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
        "\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            resnet.conv1,\n",
        "            resnet.bn1,\n",
        "            resnet.relu\n",
        "        )  # 64 channels\n",
        "\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            resnet.maxpool,\n",
        "            resnet.layer1\n",
        "        )\n",
        "\n",
        "        self.encoder3 = resnet.layer2\n",
        "        self.encoder4 = resnet.layer3\n",
        "        self.encoder5 = resnet.layer4\n",
        "\n",
        "        self.decoder4 = self._make_decoder_block(encoder_channels[4], encoder_channels[3])\n",
        "        self.decoder3 = self._make_decoder_block(encoder_channels[3], encoder_channels[2])\n",
        "        self.decoder2 = self._make_decoder_block(encoder_channels[2], encoder_channels[1])\n",
        "        self.decoder1 = self._make_decoder_block(encoder_channels[1], encoder_channels[0])\n",
        "\n",
        "        self.final_conv = nn.Conv2d(encoder_channels[0], n_classes, 1)\n",
        "\n",
        "        self._initialize_decoder_weights()\n",
        "\n",
        "    def _make_decoder_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 2, stride=2),\n",
        "            DoubleConv(out_channels * 2, out_channels)  # *2 for skip connection\n",
        "        )\n",
        "\n",
        "    def _initialize_decoder_weights(self):\n",
        "        for m in [self.decoder4, self.decoder3, self.decoder2, self.decoder1, self.final_conv]:\n",
        "            for module in m.modules():\n",
        "                if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "                elif isinstance(module, nn.BatchNorm2d):\n",
        "                    nn.init.constant_(module.weight, 1)\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder with skip connections\n",
        "        e1 = self.encoder1(x)      # 64, H/2, W/2\n",
        "        e2 = self.encoder2(e1)     # 256, H/4, W/4\n",
        "        e3 = self.encoder3(e2)     # 512, H/8, W/8\n",
        "        e4 = self.encoder4(e3)     # 1024, H/16, W/16\n",
        "        e5 = self.encoder5(e4)     # 2048, H/32, W/32\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        d4 = self.decoder4[0](e5)  # Upsample\n",
        "        d4 = torch.cat([d4, e4], dim=1)  # Skip connection\n",
        "        d4 = self.decoder4[1](d4)  # Double conv\n",
        "\n",
        "        d3 = self.decoder3[0](d4)\n",
        "        d3 = torch.cat([d3, e3], dim=1)\n",
        "        d3 = self.decoder3[1](d3)\n",
        "\n",
        "        d2 = self.decoder2[0](d3)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.decoder2[1](d2)\n",
        "\n",
        "        d1 = self.decoder1[0](d2)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.decoder1[1](d1)\n",
        "\n",
        "        d1 = F.interpolate(d1, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return self.final_conv(d1)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"Original U-Net implementation (from scratch)\"\"\"\n",
        "    def __init__(self, n_channels=3, n_classes=21):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Encoder\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n",
        "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n",
        "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512))\n",
        "        self.down4 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(512, 1024))\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.conv1 = DoubleConv(512 + 512, 512)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.conv2 = DoubleConv(256 + 256, 256)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.conv3 = DoubleConv(128 + 128, 128)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.conv4 = DoubleConv(64 + 64, 64)\n",
        "\n",
        "        # Final output layer\n",
        "        self.outc = nn.Conv2d(64, n_classes, 1)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        x = self.up1(x5)\n",
        "        if x.shape[-2:] != x4.shape[-2:]:\n",
        "            x = F.interpolate(x, size=x4.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x4, x], dim=1)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.up2(x)\n",
        "        if x.shape[-2:] != x3.shape[-2:]:\n",
        "            x = F.interpolate(x, size=x3.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x3, x], dim=1)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = self.up3(x)\n",
        "        if x.shape[-2:] != x2.shape[-2:]:\n",
        "            x = F.interpolate(x, size=x2.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x2, x], dim=1)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        x = self.up4(x)\n",
        "        if x.shape[-2:] != x1.shape[-2:]:\n",
        "            x = F.interpolate(x, size=x1.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x1, x], dim=1)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        return self.outc(x)\n",
        "\n",
        "class VOCDataset(Dataset):\n",
        "    def __init__(self, split='train', transform=None, target_transform=None):\n",
        "        self.dataset = VOCSegmentation(\n",
        "            root='./data',\n",
        "            year='2012',\n",
        "            image_set=split,\n",
        "            download=True,\n",
        "            transform=transform,\n",
        "            target_transform=target_transform\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, target = self.dataset[idx]\n",
        "\n",
        "        if not isinstance(image, torch.Tensor):\n",
        "            image = transforms.ToTensor()(image)\n",
        "\n",
        "        if not isinstance(target, torch.Tensor):\n",
        "            target = torch.from_numpy(np.array(target)).long()\n",
        "            target[target == 255] = 0\n",
        "\n",
        "        return image, target\n",
        "\n",
        "def get_transforms():\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    target_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256), interpolation=Image.NEAREST),\n",
        "    ])\n",
        "\n",
        "    return train_transform, target_transform\n",
        "\n",
        "class IoUMetrics:\n",
        "    def __init__(self, num_classes, device):\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.intersection = torch.zeros(self.num_classes, device=self.device)\n",
        "        self.union = torch.zeros(self.num_classes, device=self.device)\n",
        "        self.total_pixels = 0\n",
        "        self.correct_pixels = 0\n",
        "\n",
        "    def update(self, pred, target):\n",
        "        pred = pred.flatten()\n",
        "        target = target.flatten()\n",
        "\n",
        "        valid_mask = target != 255\n",
        "        pred = pred[valid_mask]\n",
        "        target = target[valid_mask]\n",
        "\n",
        "        self.correct_pixels += (pred == target).sum().item()\n",
        "        self.total_pixels += target.numel()\n",
        "\n",
        "        for cls in range(self.num_classes):\n",
        "            pred_cls = (pred == cls)\n",
        "            target_cls = (target == cls)\n",
        "\n",
        "            intersection = (pred_cls & target_cls).sum().float()\n",
        "            union = (pred_cls | target_cls).sum().float()\n",
        "\n",
        "            self.intersection[cls] += intersection\n",
        "            self.union[cls] += union\n",
        "\n",
        "    def compute(self):\n",
        "        iou_per_class = self.intersection / (self.union + 1e-8)\n",
        "        valid_classes = self.union > 0\n",
        "        mean_iou = iou_per_class[valid_classes].mean()\n",
        "        pixel_accuracy = self.correct_pixels / (self.total_pixels + 1e-8)\n",
        "\n",
        "        return {\n",
        "            'mean_iou': mean_iou.item(),\n",
        "            'iou_per_class': iou_per_class.cpu().numpy(),\n",
        "            'pixel_accuracy': pixel_accuracy,\n",
        "            'valid_classes': valid_classes.cpu().numpy()\n",
        "        }\n",
        "\n",
        "def get_memory_usage():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated(device) / 1024 / 1024\n",
        "    else:\n",
        "        process = psutil.Process(os.getpid())\n",
        "        return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device, model_name, num_classes=21):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    iou_metrics = IoUMetrics(num_classes, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            if 'segformer' in model_name.lower():\n",
        "                outputs = model(pixel_values=images)\n",
        "                logits = outputs.logits\n",
        "            else:\n",
        "                logits = model(images)\n",
        "\n",
        "            logits = F.interpolate(logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
        "            loss = criterion(logits, masks)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            pred_masks = torch.argmax(logits, dim=1)\n",
        "            iou_metrics.update(pred_masks, masks)\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    metrics = iou_metrics.compute()\n",
        "\n",
        "    return avg_loss, metrics\n",
        "\n",
        "def train_epoch(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epoch, model_name, num_classes=21):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    start_memory = get_memory_usage()\n",
        "\n",
        "    for i, (images, masks) in enumerate(train_loader):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if 'segformer' in model_name.lower():\n",
        "            outputs = model(pixel_values=images)\n",
        "            logits = outputs.logits\n",
        "            logits = F.interpolate(logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        else:\n",
        "            logits = model(images)\n",
        "            logits = F.interpolate(logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        loss = criterion(logits, masks)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        if 'unet' in model_name.lower():\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f'Epoch {epoch+1}, Batch {i}, Loss: {loss.item():.4f}, LR: {current_lr:.6f}')\n",
        "\n",
        "    # Step scheduler after epoch\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    epoch_time = time.time() - start_time\n",
        "    end_memory = get_memory_usage()\n",
        "    memory_used = end_memory - start_memory\n",
        "\n",
        "    print(f\"Evaluating {model_name} on validation set...\")\n",
        "    val_loss, val_metrics = evaluate_model(model, val_loader, criterion, device, model_name, num_classes)\n",
        "    print(f\"Validation - Loss: {val_loss:.4f}, mIoU: {val_metrics['mean_iou']:.4f}, \"\n",
        "          f\"Pixel Acc: {val_metrics['pixel_accuracy']:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'val_metrics': val_metrics,\n",
        "        'epoch_time': epoch_time,\n",
        "        'memory_used': memory_used\n",
        "    }\n",
        "\n",
        "def visualize_predictions(model, dataloader, device, epoch, model_name, save_path, num_classes=21):\n",
        "    model.eval()\n",
        "    data_iter = iter(dataloader)\n",
        "    images, true_masks = next(data_iter)\n",
        "    image = images[0:1].to(device)\n",
        "    true_mask = true_masks[0].cpu().numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if 'segformer' in model_name.lower():\n",
        "            outputs = model(pixel_values=image)\n",
        "            logits = outputs.logits\n",
        "        else:\n",
        "            logits = model(image)\n",
        "\n",
        "        logits = F.interpolate(logits, size=true_mask.shape, mode='bilinear', align_corners=False)\n",
        "        pred_mask = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "    iou_metrics = IoUMetrics(num_classes, device='cpu')\n",
        "    iou_metrics.update(torch.tensor(pred_mask), torch.tensor(true_mask))\n",
        "    metrics = iou_metrics.compute()\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    original_img = images[0].cpu()\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    original_img = torch.clamp(original_img * std + mean, 0, 1)\n",
        "\n",
        "    axes[0].imshow(original_img.permute(1, 2, 0))\n",
        "    axes[0].set_title('Original Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(true_mask, cmap='tab20')\n",
        "    axes[1].set_title('Ground Truth Mask')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(pred_mask, cmap='tab20')\n",
        "    axes[2].set_title(f'{model_name} - Epoch {epoch+1}\\n'\n",
        "                     f'mIoU: {metrics[\"mean_iou\"]:.3f}, '\n",
        "                     f'Pixel Acc: {metrics[\"pixel_accuracy\"]:.3f}')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_path}/{model_name}_epoch_{epoch+1}_prediction.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def setup_model_training(model_name, model, freeze_encoder=False):\n",
        "    \"\"\"Setup training configuration for different models\"\"\"\n",
        "    if model_name == 'Pretrained U-Net (ResNet50)':\n",
        "        if freeze_encoder:\n",
        "            # Freeze encoder (ResNet backbone) for initial training\n",
        "            for name, param in model.named_parameters():\n",
        "                if 'encoder' in name:\n",
        "                    param.requires_grad = False\n",
        "            lr = 1e-3  # Higher LR for decoder only\n",
        "            print(f\"Frozen encoder parameters in {model_name}\")\n",
        "        else:\n",
        "            # Fine-tune entire model\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = True\n",
        "            lr = 5e-4  # Lower LR for fine-tuning\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
        "\n",
        "    elif model_name == 'U-Net (From Scratch)':\n",
        "        lr = 6e-4\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
        "\n",
        "    else:  # SegFormer\n",
        "        lr = 1e-4\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        scheduler = None\n",
        "\n",
        "    return optimizer, scheduler\n",
        "\n",
        "def run_comparison():\n",
        "    train_transform, target_transform = get_transforms()\n",
        "\n",
        "    print(\"Loading VOC2012 dataset...\")\n",
        "    train_dataset = VOCDataset('train', train_transform, target_transform)\n",
        "    val_dataset = VOCDataset('val', train_transform, target_transform)\n",
        "\n",
        "    batch_size = 16\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    os.makedirs('segmentation_results', exist_ok=True)\n",
        "\n",
        "    print(\"\\nInitializing models...\")\n",
        "\n",
        "    # Initialize models\n",
        "    unet_scratch = UNet(n_channels=3, n_classes=21).to(device)\n",
        "    unet_pretrained_resnet50 = PretrainedUNet(n_classes=21, backbone='resnet50', pretrained=True).to(device)\n",
        "\n",
        "    segformer_model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "        \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
        "        num_labels=21,\n",
        "        ignore_mismatched_sizes=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Count parameters\n",
        "    models_info = [\n",
        "        #('U-Net (From Scratch)', unet_scratch),\n",
        "        ('Pretrained U-Net (ResNet50)', unet_pretrained_resnet50),\n",
        "        #('SegFormer', segformer_model)\n",
        "    ]\n",
        "\n",
        "    for name, model in models_info:\n",
        "        params = count_parameters(model)\n",
        "        print(f\"{name} parameters: {params:,} ({params/1e6:.1f}M)\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "    epochs = 30\n",
        "    num_classes = 21\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model_name, model in models_info:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training {model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        optimizer, scheduler = setup_model_training(model_name, model)\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        val_mious = []\n",
        "        val_pixel_accs = []\n",
        "        train_times = []\n",
        "        memory_usage = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "            epoch_results = train_epoch(\n",
        "                model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                device, epoch, model_name, num_classes\n",
        "            )\n",
        "\n",
        "            train_losses.append(epoch_results['train_loss'])\n",
        "            val_losses.append(epoch_results['val_loss'])\n",
        "            val_mious.append(epoch_results['val_metrics']['mean_iou'])\n",
        "            val_pixel_accs.append(epoch_results['val_metrics']['pixel_accuracy'])\n",
        "            train_times.append(epoch_results['epoch_time'])\n",
        "            memory_usage.append(epoch_results['memory_used'])\n",
        "\n",
        "            visualize_predictions(model, val_loader, device, epoch, model_name, 'segmentation_results', num_classes)\n",
        "\n",
        "            print(f\"Train Loss: {epoch_results['train_loss']:.4f}, \"\n",
        "                  f\"Val Loss: {epoch_results['val_loss']:.4f}, \"\n",
        "                  f\"Time: {epoch_results['epoch_time']:.2f}s, \"\n",
        "                  f\"Memory: {epoch_results['memory_used']:.2f}MB\")\n",
        "\n",
        "        param_count = count_parameters(model)\n",
        "        results[model_name] = {\n",
        "            'parameters': param_count,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'val_mious': val_mious,\n",
        "            'val_pixel_accs': val_pixel_accs,\n",
        "            'train_times': train_times,\n",
        "            'memory_usage': memory_usage,\n",
        "            'avg_time_per_epoch': np.mean(train_times),\n",
        "            'avg_memory_per_epoch': np.mean(memory_usage),\n",
        "            'final_train_loss': train_losses[-1],\n",
        "            'final_val_loss': val_losses[-1],\n",
        "            'best_miou': max(val_mious),\n",
        "            'final_miou': val_mious[-1],\n",
        "            'best_pixel_acc': max(val_pixel_accs),\n",
        "            'final_pixel_acc': val_pixel_accs[-1]\n",
        "        }\n",
        "\n",
        "\n",
        "    print(\"\\nGenerating comparison plots...\")\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(24, 12))\n",
        "\n",
        "    colors = ['blue', 'green', 'red', 'orange']\n",
        "\n",
        "    # Loss comparison\n",
        "    for i, model_name in enumerate(results.keys()):\n",
        "        axes[0, 0].plot(results[model_name]['train_losses'],\n",
        "                       label=f'{model_name} (Train)', marker='o', color=colors[i])\n",
        "        axes[0, 0].plot(results[model_name]['val_losses'],\n",
        "                       label=f'{model_name} (Val)', marker='s', linestyle='--', color=colors[i])\n",
        "    axes[0, 0].set_title('Loss Comparison')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    # IoU comparison\n",
        "    for i, model_name in enumerate(results.keys()):\n",
        "        axes[0, 1].plot(results[model_name]['val_mious'],\n",
        "                       label=model_name, marker='o', color=colors[i])\n",
        "    axes[0, 1].set_title('Mean IoU Comparison')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Mean IoU')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    # Pixel Accuracy comparison\n",
        "    for i, model_name in enumerate(results.keys()):\n",
        "        axes[0, 2].plot(results[model_name]['val_pixel_accs'],\n",
        "                       label=model_name, marker='o', color=colors[i])\n",
        "    axes[0, 2].set_title('Pixel Accuracy Comparison')\n",
        "    axes[0, 2].set_xlabel('Epoch')\n",
        "    axes[0, 2].set_ylabel('Pixel Accuracy')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True)\n",
        "\n",
        "    # Time comparison\n",
        "    model_names = list(results.keys())\n",
        "    times = [results[name]['avg_time_per_epoch'] for name in model_names]\n",
        "    bars = axes[1, 0].bar(range(len(model_names)), times, color=colors[:len(model_names)])\n",
        "    axes[1, 0].set_title('Average Training Time per Epoch')\n",
        "    axes[1, 0].set_ylabel('Time (seconds)')\n",
        "    axes[1, 0].set_xticks(range(len(model_names)))\n",
        "    axes[1, 0].set_xticklabels([name.replace(' ', '\\n') for name in model_names], rotation=0)\n",
        "    for i, v in enumerate(times):\n",
        "        axes[1, 0].text(i, v + max(times)*0.01, f'{v:.1f}s', ha='center')\n",
        "\n",
        "    # Memory comparison\n",
        "    memory = [results[name]['avg_memory_per_epoch'] for name in model_names]\n",
        "    bars = axes[1, 1].bar(range(len(model_names)), memory, color=colors[:len(model_names)])\n",
        "    axes[1, 1].set_title('Average Memory Usage per Epoch')\n",
        "    axes[1, 1].set_ylabel('Memory (MB)')\n",
        "    axes[1, 1].set_xticks(range(len(model_names)))\n",
        "    axes[1, 1].set_xticklabels([name.replace(' ', '\\n') for name in model_names], rotation=0)\n",
        "    for i, v in enumerate(memory):\n",
        "        axes[1, 1].text(i, v + max(memory)*0.01, f'{v:.1f}MB', ha='center')\n",
        "\n",
        "    # Parameters comparison\n",
        "    params = [results[name]['parameters'] for name in model_names]\n",
        "    axes[1, 2].bar(model_names, params, color=['purple', 'brown'])\n",
        "    axes[1, 2].set_title('Model Parameters Comparison')\n",
        "    axes[1, 2].set_ylabel('Number of Parameters')\n",
        "    for i, v in enumerate(params):\n",
        "        axes[1, 2].text(i, v + max(params)*0.01, f'{v/1e6:.1f}M', ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('segmentation_results/comprehensive_model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Create IoU metrics summary plot\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.35\n",
        "\n",
        "    best_mious = [results[name]['best_miou'] for name in model_names]\n",
        "    final_mious = [results[name]['final_miou'] for name in model_names]\n",
        "\n",
        "    bars1 = ax.bar(x - width/2, best_mious, width, label='Best mIoU', color='skyblue')\n",
        "    bars2 = ax.bar(x + width/2, final_mious, width, label='Final mIoU', color='lightcoral')\n",
        "\n",
        "    ax.set_xlabel('Models')\n",
        "    ax.set_ylabel('Mean IoU')\n",
        "    ax.set_title('IoU Performance Comparison')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(model_names)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "                f'{height:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "                f'{height:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('segmentation_results/iou_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for model_name in results.keys():\n",
        "        result = results[model_name]\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  Parameters: {result['parameters']:,} ({result['parameters']/1e6:.1f}M)\")\n",
        "        print(f\"  Final Train Loss: {result['final_train_loss']:.4f}\")\n",
        "        print(f\"  Final Val Loss: {result['final_val_loss']:.4f}\")\n",
        "        print(f\"  Best mIoU: {result['best_miou']:.4f}\")\n",
        "        print(f\"  Final mIoU: {result['final_miou']:.4f}\")\n",
        "        print(f\"  Best Pixel Accuracy: {result['best_pixel_acc']:.4f}\")\n",
        "        print(f\"  Final Pixel Accuracy: {result['final_pixel_acc']:.4f}\")\n",
        "        print(f\"  Avg Time/Epoch: {result['avg_time_per_epoch']:.2f}s\")\n",
        "        print(f\"  Avg Memory/Epoch: {result['avg_memory_per_epoch']:.2f}MB\")\n",
        "\n",
        "    print(f\"\\nDataset: PASCAL VOC 2012 (21 classes)\")\n",
        "    print(f\"Input Resolution: 256x256\")\n",
        "    print(f\"Batch Size: {batch_size}\")\n",
        "    print(f\"Training Epochs: {epochs}\")\n",
        "    print(f\"Evaluation Metrics: Loss, Mean IoU, Pixel Accuracy\")\n",
        "\n",
        "    # Performance ranking\n",
        "    print(f\"\\n\" + \"=\"*50)\n",
        "    print(\"PERFORMANCE RANKING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Sort by best mIoU\n",
        "    sorted_by_miou = sorted(results.items(), key=lambda x: x[1]['best_miou'], reverse=True)\n",
        "    print(\"\\nBy Best Mean IoU:\")\n",
        "    for i, (model_name, result) in enumerate(sorted_by_miou, 1):\n",
        "        print(f\"  {i}. {model_name}: {result['best_miou']:.4f}\")\n",
        "\n",
        "    # Sort by efficiency (mIoU per parameter)\n",
        "    efficiency_scores = {name: result['best_miou'] / (result['parameters'] / 1e6)\n",
        "                        for name, result in results.items()}\n",
        "    sorted_by_efficiency = sorted(efficiency_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(\"\\nBy Efficiency (mIoU per Million Parameters):\")\n",
        "    for i, (model_name, score) in enumerate(sorted_by_efficiency, 1):\n",
        "        print(f\"  {i}. {model_name}: {score:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_comparison()"
      ],
      "metadata": {
        "id": "-CD2ZfJqNSum"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}